{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with TensorFlow: Softmax only\n",
    "\n",
    "Date: August 21-22, 2018\n",
    "\n",
    "Neural network: Softmax only\n",
    "\n",
    "Classification of human body motion:\n",
    "- walking\n",
    "- sitting down\n",
    "- turning right while walking\n",
    "- turning left while walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as sg\n",
    "import serial\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data set for training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUMS = 260  # sec\n",
    "SAMPLING_RATE = 80.0  # Hz\n",
    "GYRO_RESOLUTION = 250.0 / 32768.0\n",
    "ACCEL_RESOLUTION = 2.0 / 32768.0\n",
    "\n",
    "FILE0 = './20180821_walking_straight.csv'\n",
    "FILE1 = './20180821_sitting.csv'\n",
    "FILE2 = './20180821_turning_left.csv'\n",
    "FILE3 = './20180821_turning_right.csv'\n",
    "\n",
    "FILE0_T = './20180822_walking_straight.csv'\n",
    "FILE1_T = './20180822_sitting.csv'\n",
    "FILE2_T = './20180822_turning_left.csv'\n",
    "FILE3_T = './20180822_turning_right.csv'\n",
    "\n",
    "TIME_INTERVAL = 260.0 / SAMPLING_RATE  # sec\n",
    "\n",
    "MEASUREMENTS = 8\n",
    "RECORDS = 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "gres = lambda v: v * GYRO_RESOLUTION\n",
    "ares = lambda v: v * ACCEL_RESOLUTION\n",
    "to_time = lambda v: v / SAMPLING_RATE\n",
    "\n",
    "def conv(df):\n",
    "    df[['gx', 'gy', 'gz']] = df[['gx', 'gy', 'gz']].apply(gres)\n",
    "    df[['ax', 'ay', 'az']] = df[['ax', 'ay', 'az']].apply(ares)\n",
    "    CUTOFF = 10.0\n",
    "    b, a = sg.butter(5, CUTOFF/SAMPLING_RATE, btype='low')\n",
    "    df[['ax', 'ay', 'az']] = df[['ax', 'ay', 'az']].apply(lambda row: sg.lfilter(b, a, row))\n",
    "    CUTOFF = 10.0\n",
    "    b, a = sg.butter(5, CUTOFF/SAMPLING_RATE, btype='low')\n",
    "    df[['gx', 'gy', 'gz']] = df[['gx', 'gy', 'gz']].apply(lambda row: sg.lfilter(b, a, row))\n",
    "    df[['cnt']] = df[['cnt']].apply(to_time)\n",
    "    df.set_index('cnt', drop=True, inplace=True)\n",
    "    # measurements = df.tail(1).iloc[0,0] + 1\n",
    "    # return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0,df0_t = pd.read_csv(FILE0, dtype=np.int16),pd.read_csv(FILE0_T, dtype=np.int16)\n",
    "df1,df1_t = pd.read_csv(FILE1, dtype=np.int16),pd.read_csv(FILE1_T, dtype=np.int16)\n",
    "df2,df2_t = pd.read_csv(FILE2, dtype=np.int16),pd.read_csv(FILE2_T, dtype=np.int16)\n",
    "df3,df3_t = pd.read_csv(FILE3, dtype=np.int16),pd.read_csv(FILE3_T, dtype=np.int16)\n",
    "\n",
    "conv(df0), conv(df0_t)\n",
    "conv(df1), conv(df1_t)\n",
    "conv(df2), conv(df2_t)\n",
    "conv(df3), conv(df3_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_set, df0set, df1set, df2set, df3set = [], [], [], [], []\n",
    "df_t_set, df0tset, df1tset, df2tset, df3tset = [], [], [], [], []\n",
    "#df_set, df0set, df1set, df2set = [], [], [], []\n",
    "#df_set, df0set, df1set = [], [], []\n",
    "for i in range(MEASUREMENTS):\n",
    "    df0set.append([df0[df0['id'] == i], [1,0,0,0]])\n",
    "    df1set.append([df1[df1['id'] == i], [0,1,0,0]])\n",
    "    df2set.append([df2[df2['id'] == i], [0,0,1,0]])\n",
    "    df3set.append([df3[df3['id'] == i], [0,0,0,1]])\n",
    "    df0tset.append([df0_t[df0_t['id'] == i], [1,0,0,0]])\n",
    "    df1tset.append([df1_t[df1_t['id'] == i], [0,1,0,0]])\n",
    "    df2tset.append([df2_t[df2_t['id'] == i], [0,0,1,0]])\n",
    "    df3tset.append([df3_t[df3_t['id'] == i], [0,0,0,1]])\n",
    "\n",
    "df_set.extend(df0set)\n",
    "df_set.extend(df1set)\n",
    "df_set.extend(df2set)\n",
    "df_set.extend(df3set)\n",
    "\n",
    "df_t_set.extend(df0tset)\n",
    "df_t_set.extend(df1tset)\n",
    "df_t_set.extend(df2tset)\n",
    "df_t_set.extend(df3tset)\n",
    "\n",
    "random.shuffle(df_set)\n",
    "random.shuffle(df_t_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use accel x axis and gyro z axis values for classification of human body motion\n",
    "train_x, train_t_x = [], []\n",
    "train_t, train_t_t = [], []\n",
    "for df, label in df_set:\n",
    "    values = np.concatenate((df['ax'].values, df['ay'].values, df['az'].values,\n",
    "                             df['gx'].values, df['gy'].values, df['gz'].values),\n",
    "                            axis=None)\n",
    "    train_x.append(values)\n",
    "    train_t.append(label)\n",
    "for df, label in df_t_set:\n",
    "    values = np.concatenate((df['ax'].values, df['ay'].values, df['az'].values,\n",
    "                             df['gx'].values, df['gy'].values, df['gz'].values),\n",
    "                            axis=None)\n",
    "    train_t_x.append(values)\n",
    "    train_t_t.append(label)\n",
    "\n",
    "#train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "np.random.seed(20180822)\n",
    "tf.set_random_seed(20180822)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = SAMPLE_NUMS * 6\n",
    "num_units = 64\n",
    "num_classify = 4\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, num_samples])\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal([num_samples, num_units]))\n",
    "b1 = tf.Variable(tf.zeros([num_units]))\n",
    "hidden1 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w0 = tf.Variable(tf.zeros([num_units, num_classify]))\n",
    "b0 = tf.Variable(tf.zeros([num_classify]))\n",
    "p = tf.nn.softmax(tf.matmul(hidden1, w0) + b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.placeholder(tf.float32, [None, num_classify])\n",
    "loss = -tf.reduce_sum(t * tf.log(p))\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss: 19.662357330322266, Accuracy: 0.875\n",
      "Step: 200, Loss: 13.925043106079102, Accuracy: 0.875\n",
      "Step: 300, Loss: 12.304342269897461, Accuracy: 0.875\n",
      "Step: 400, Loss: 11.863088607788086, Accuracy: 0.875\n",
      "Step: 500, Loss: 11.458113670349121, Accuracy: 0.875\n",
      "Step: 600, Loss: 11.293909072875977, Accuracy: 0.875\n",
      "Step: 700, Loss: 11.288328170776367, Accuracy: 0.875\n",
      "Step: 800, Loss: 11.287398338317871, Accuracy: 0.875\n",
      "Step: 900, Loss: 11.304747581481934, Accuracy: 0.875\n",
      "Step: 1000, Loss: 11.334665298461914, Accuracy: 0.875\n",
      "Step: 1100, Loss: 11.394475936889648, Accuracy: 0.875\n",
      "Step: 1200, Loss: 11.459219932556152, Accuracy: 0.875\n",
      "Step: 1300, Loss: 11.523523330688477, Accuracy: 0.875\n",
      "Step: 1400, Loss: 11.58803653717041, Accuracy: 0.875\n",
      "Step: 1500, Loss: 11.635093688964844, Accuracy: 0.875\n",
      "Step: 1600, Loss: 11.320343017578125, Accuracy: 0.875\n",
      "Step: 1700, Loss: 11.359342575073242, Accuracy: 0.875\n",
      "Step: 1800, Loss: 11.3921480178833, Accuracy: 0.875\n",
      "Step: 1900, Loss: 11.432107925415039, Accuracy: 0.875\n",
      "Step: 2000, Loss: 11.474102020263672, Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in range(2000):\n",
    "    i += 1\n",
    "    sess.run(train_step, feed_dict={x:train_x, t:train_t})\n",
    "    if i % 100 == 0:\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={x:train_t_x, t: train_t_t})\n",
    "        print('Step: {}, Loss: {}, Accuracy: {}'.format(i, loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[98,  0,  0,  1],\n",
       "       [ 0, 97,  0,  1],\n",
       "       [ 0,  0, 99,  0],\n",
       "       [10,  5, 81,  1],\n",
       "       [ 0,  0, 99,  0],\n",
       "       [ 9,  1, 89,  0],\n",
       "       [ 1,  0,  0, 98],\n",
       "       [ 0,  0, 99,  0],\n",
       "       [ 0, 99,  0,  0],\n",
       "       [ 1,  0,  0, 98]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = sess.run(p, feed_dict={x:train_t_x})\n",
    "(p_test*100).astype(int)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1]]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t_t[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
