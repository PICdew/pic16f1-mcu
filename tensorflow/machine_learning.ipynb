{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with TensorFlow\n",
    "\n",
    "Classification of human body motion:\n",
    "- walking\n",
    "- sitting down\n",
    "- turning right while walking\n",
    "- turning left while walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as sg\n",
    "import serial\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data set for training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUMS = 260  # sec\n",
    "SAMPLING_RATE = 80.0  # Hz\n",
    "GYRO_RESOLUTION = 250.0 / 32768.0\n",
    "ACCEL_RESOLUTION = 2.0 / 32768.0\n",
    "\n",
    "FILE0 = './20180821_walking_straight.csv'\n",
    "FILE1 = './20180821_sitting_down.csv'\n",
    "FILE2 = './20180821_turning_left.csv'\n",
    "FILE3 = './20180821_turning_right.csv'\n",
    "\n",
    "TIME_INTERVAL = 260.0 / SAMPLING_RATE  # sec\n",
    "\n",
    "MEASUREMENTS = 8\n",
    "RECORDS = 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gres = lambda v: v * GYRO_RESOLUTION\n",
    "ares = lambda v: v * ACCEL_RESOLUTION\n",
    "to_time = lambda v: v / SAMPLING_RATE\n",
    "\n",
    "def conv(df):\n",
    "    df[['gx', 'gy', 'gz']] = df[['gx', 'gy', 'gz']].apply(gres)\n",
    "    df[['ax', 'ay', 'az']] = df[['ax', 'ay', 'az']].apply(ares)\n",
    "    CUTOFF = 10.0\n",
    "    b, a = sg.butter(5, CUTOFF/SAMPLING_RATE, btype='low')\n",
    "    df[['ax', 'ay', 'az']] = df[['ax', 'ay', 'az']].apply(lambda row: sg.lfilter(b, a, row))\n",
    "    CUTOFF = 10.0\n",
    "    b, a = sg.butter(5, CUTOFF/SAMPLING_RATE, btype='low')\n",
    "    df[['gx', 'gy', 'gz']] = df[['gx', 'gy', 'gz']].apply(lambda row: sg.lfilter(b, a, row))\n",
    "    df[['cnt']] = df[['cnt']].apply(to_time)\n",
    "    df.set_index('cnt', drop=True, inplace=True)\n",
    "    # measurements = df.tail(1).iloc[0,0] + 1\n",
    "    # return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(FILE0, dtype=np.int16)\n",
    "df1 = pd.read_csv(FILE1, dtype=np.int16)\n",
    "df2 = pd.read_csv(FILE2, dtype=np.int16)\n",
    "df3 = pd.read_csv(FILE3, dtype=np.int16)\n",
    "\n",
    "conv(df0)\n",
    "conv(df1)\n",
    "conv(df2)\n",
    "conv(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df0['label'] = np.full(RECORDS*MEASUREMENTS, 0)\n",
    "df1['label'] = np.full(RECORDS*MEASUREMENTS, 1)\n",
    "df2['label'] = np.full(RECORDS*MEASUREMENTS, 2)\n",
    "df3['label'] = np.full(RECORDS*MEASUREMENTS, 2)\n",
    "\n",
    "df_set, df0set, df1set, df2set, df3set = [], [], [], [], []\n",
    "#df_set, df0set, df1set, df2set = [], [], [], []\n",
    "#df_set, df0set, df1set = [], [], []\n",
    "for i in range(MEASUREMENTS):\n",
    "    df0set.append([df0[df0['id'] == i], [1,0,0,0]])\n",
    "    df1set.append([df1[df1['id'] == i], [0,1,0,0]])\n",
    "    df2set.append([df2[df2['id'] == i], [0,0,1,0]])\n",
    "    df3set.append([df3[df3['id'] == i], [0,0,0,1]])\n",
    "\n",
    "df_set.extend(df0set)\n",
    "df_set.extend(df1set)\n",
    "df_set.extend(df2set)\n",
    "df_set.extend(df3set)\n",
    "\n",
    "random.shuffle(df_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use accel x axis and gyro z axis values for classification of human body motion\n",
    "train_x = []\n",
    "train_t = []\n",
    "for df, label in df_set:\n",
    "    values = np.concatenate((df['ax'].values, df['ay'].values, df['az'].values,\n",
    "                             df['gx'].values, df['gy'].values, df['gz'].values),\n",
    "                            axis=None)\n",
    "    train_x.append(values)\n",
    "    train_t.append(label)\n",
    "#train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "np.random.seed(20180821)\n",
    "tf.set_random_seed(20180821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = SAMPLE_NUMS * 6\n",
    "num_units = 16\n",
    "num_classify = 4\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, num_samples])\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal([num_samples, num_units]))\n",
    "b1 = tf.Variable(tf.zeros([num_units]))\n",
    "hidden1 = tf.nn.tanh(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w0 = tf.Variable(tf.zeros([num_units, num_classify]))\n",
    "b0 = tf.Variable(tf.zeros([num_classify]))\n",
    "p = tf.nn.softmax(tf.matmul(hidden1, w0) + b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.placeholder(tf.float32, [None, num_classify])\n",
    "loss = -tf.reduce_sum(t * tf.log(p))\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss: 22.8950138092041, Accuracy: 0.9375\n",
      "Step: 200, Loss: 14.763388633728027, Accuracy: 0.9375\n",
      "Step: 300, Loss: 10.783812522888184, Accuracy: 0.96875\n",
      "Step: 400, Loss: 8.331635475158691, Accuracy: 1.0\n",
      "Step: 500, Loss: 6.659186363220215, Accuracy: 1.0\n",
      "Step: 600, Loss: 5.4530439376831055, Accuracy: 1.0\n",
      "Step: 700, Loss: 4.550124168395996, Accuracy: 1.0\n",
      "Step: 800, Loss: 3.85486102104187, Accuracy: 1.0\n",
      "Step: 900, Loss: 3.307219982147217, Accuracy: 1.0\n",
      "Step: 1000, Loss: 2.867664337158203, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in range(1000):\n",
    "    i += 1\n",
    "    sess.run(train_step, feed_dict={x:train_x, t:train_t})\n",
    "    if i % 100 == 0:\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={x:train_x, t: train_t})\n",
    "        print('Step: {}, Loss: {}, Accuracy: {}'.format(i, loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.3863441e-01, 1.4137013e-01, 7.1749747e-02, 4.8245762e-02],\n",
       "       [9.0220571e-02, 1.5913935e-02, 8.7194520e-01, 2.1920275e-02],\n",
       "       [2.4960682e-02, 2.1515802e-02, 9.5350909e-01, 1.4491593e-05],\n",
       "       [1.1360802e-01, 1.1436796e-02, 1.1609067e-03, 8.7379420e-01],\n",
       "       [8.3643097e-01, 1.3018120e-02, 1.4773530e-03, 1.4907362e-01],\n",
       "       [5.4507036e-02, 7.3308852e-03, 9.3810415e-01, 5.7910005e-05],\n",
       "       [9.9289171e-02, 8.5185558e-01, 4.5609500e-02, 3.2456906e-03],\n",
       "       [5.0328404e-02, 7.5505916e-03, 7.7807467e-04, 9.4134295e-01],\n",
       "       [3.7081975e-02, 5.2375104e-03, 9.5766115e-01, 1.9418490e-05],\n",
       "       [3.7081975e-02, 5.2375104e-03, 9.5766115e-01, 1.9418490e-05],\n",
       "       [1.3779462e-03, 9.7538322e-01, 1.0149395e-02, 1.3089452e-02],\n",
       "       [2.1676442e-02, 1.0426490e-03, 5.0926064e-05, 9.7723001e-01],\n",
       "       [3.5040584e-04, 9.8836190e-01, 8.5143717e-03, 2.7732803e-03],\n",
       "       [7.3999397e-02, 9.0090483e-01, 2.1517053e-02, 3.5786885e-03],\n",
       "       [1.2467082e-01, 1.5539097e-03, 1.2849626e-04, 8.7364680e-01],\n",
       "       [1.7802063e-02, 9.3649638e-01, 7.1668375e-04, 4.4984803e-02],\n",
       "       [8.7053931e-01, 2.6628734e-03, 7.9882227e-02, 4.6915539e-02],\n",
       "       [9.2505682e-03, 4.8500192e-03, 7.5482283e-05, 9.8582387e-01],\n",
       "       [7.4996316e-01, 7.2542973e-02, 9.6743880e-03, 1.6781946e-01],\n",
       "       [9.1994011e-01, 2.7553784e-03, 3.6935389e-02, 4.0369164e-02],\n",
       "       [2.4960682e-02, 2.1515802e-02, 9.5350909e-01, 1.4491593e-05],\n",
       "       [8.7736040e-02, 8.2299225e-03, 9.0375608e-01, 2.7794112e-04],\n",
       "       [2.8580648e-03, 9.8396021e-01, 1.2205236e-02, 9.7651145e-04],\n",
       "       [1.9655093e-03, 9.7665000e-01, 8.7744771e-03, 1.2610062e-02],\n",
       "       [9.5639801e-01, 2.5193703e-03, 4.0922642e-02, 1.5990583e-04],\n",
       "       [2.9020827e-02, 4.9685845e-03, 4.6403680e-04, 9.6554661e-01],\n",
       "       [9.6925628e-01, 3.1544134e-04, 2.9943502e-02, 4.8475753e-04],\n",
       "       [2.4535973e-02, 1.2734924e-02, 9.6256137e-01, 1.6770340e-04],\n",
       "       [2.1676442e-02, 1.0426490e-03, 5.0926064e-05, 9.7723001e-01],\n",
       "       [7.9737884e-01, 2.7289448e-02, 1.1839762e-01, 5.6934111e-02],\n",
       "       [5.5659268e-02, 9.2603102e-02, 1.5260282e-04, 8.5158503e-01],\n",
       "       [2.2617256e-04, 9.4288880e-01, 3.7424166e-03, 5.3142693e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = sess.run(p, feed_dict={x:train_x})\n",
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
